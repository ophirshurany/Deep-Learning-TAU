{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2129,"status":"ok","timestamp":1608068775243,"user":{"displayName":"Artem Mironov","photoUrl":"","userId":"07007599338275803513"},"user_tz":-120},"id":"GXERO9fOe0fM"},"outputs":[],"source":["import tensorflow as tf\r\n","import numpy as np\r\n","import pickle\r\n","import time\r\n","import math\r\n","import datetime\r\n","import collections\r\n","import matplotlib.pyplot as plt\r\n","\r\n","from tensorflow.keras.models import Sequential\r\n","from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Activation\r\n","from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, LearningRateScheduler, ModelCheckpoint\r\n","import tensorflow.keras.backend as K"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3896,"status":"ok","timestamp":1608068777040,"user":{"displayName":"Artem Mironov","photoUrl":"","userId":"07007599338275803513"},"user_tz":-120},"id":"Bq-d5bkHnk9u","outputId":"a954ec87-de6b-4a2f-9617-db7fd663a7bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2020-12-15 21:46:14--  https://www.dropbox.com/s/bs9ztyq27sxa3l7/ptb.zip?dl=0\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.18, 2620:100:6019:18::a27d:412\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/bs9ztyq27sxa3l7/ptb.zip [following]\n","--2020-12-15 21:46:15--  https://www.dropbox.com/s/raw/bs9ztyq27sxa3l7/ptb.zip\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uce32eb77e8f1cb88bb78a8d49f3.dl.dropboxusercontent.com/cd/0/inline/BFJ2kKkH1V1zh5nYi0rGXrokZyozB56jqDu2nz8uJ8ff6fGQS82KIz6LXubgAnbmiidTdYYuw-XSVlwcfXz1u0qFOi_dh2HkhJG0LYM77yVq_0L8muzX9hB00JgBQJIE3FI/file# [following]\n","--2020-12-15 21:46:15--  https://uce32eb77e8f1cb88bb78a8d49f3.dl.dropboxusercontent.com/cd/0/inline/BFJ2kKkH1V1zh5nYi0rGXrokZyozB56jqDu2nz8uJ8ff6fGQS82KIz6LXubgAnbmiidTdYYuw-XSVlwcfXz1u0qFOi_dh2HkhJG0LYM77yVq_0L8muzX9hB00JgBQJIE3FI/file\n","Resolving uce32eb77e8f1cb88bb78a8d49f3.dl.dropboxusercontent.com (uce32eb77e8f1cb88bb78a8d49f3.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:6019:15::a27d:40f\n","Connecting to uce32eb77e8f1cb88bb78a8d49f3.dl.dropboxusercontent.com (uce32eb77e8f1cb88bb78a8d49f3.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /cd/0/inline2/BFJ44n5PwCMBhF6PX-iLQ-PxJHaheekijRLDgbu16F052p9olp2NDVykqojVtLI_Bo5ZOHDBt8wzrXYx90r9nwXkzQZO4---_O_5hYdkU4CbsERL4oW79QTJgdfVsqUMH0KWbrYt4ysUUzYnUb3pGTI-6EvQaaizzUDZBPOj60_2O6UMAGwSLklKfBjlymOFAPTOZHPxbrJtHWOcwjiSjVXknAd9k1bmfOh41180FiCu9JllPQchWcKKh7QHiK7eF9HcYT1hLplrfdVgLPzYhpSKUJPlEh4BoVMPRrPaYitvkHsdH2_j8zNLcRCcxNi_XmWTB5UVj4_oYHOTt9ONklLrlYbmBfBlNvhtzZnHwlDxOw/file [following]\n","--2020-12-15 21:46:15--  https://uce32eb77e8f1cb88bb78a8d49f3.dl.dropboxusercontent.com/cd/0/inline2/BFJ44n5PwCMBhF6PX-iLQ-PxJHaheekijRLDgbu16F052p9olp2NDVykqojVtLI_Bo5ZOHDBt8wzrXYx90r9nwXkzQZO4---_O_5hYdkU4CbsERL4oW79QTJgdfVsqUMH0KWbrYt4ysUUzYnUb3pGTI-6EvQaaizzUDZBPOj60_2O6UMAGwSLklKfBjlymOFAPTOZHPxbrJtHWOcwjiSjVXknAd9k1bmfOh41180FiCu9JllPQchWcKKh7QHiK7eF9HcYT1hLplrfdVgLPzYhpSKUJPlEh4BoVMPRrPaYitvkHsdH2_j8zNLcRCcxNi_XmWTB5UVj4_oYHOTt9ONklLrlYbmBfBlNvhtzZnHwlDxOw/file\n","Reusing existing connection to uce32eb77e8f1cb88bb78a8d49f3.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1958845 (1.9M) [application/zip]\n","Saving to: ‘ptb.zip’\n","\n","ptb.zip             100%[===================\u003e]   1.87M  --.-KB/s    in 0.03s   \n","\n","2020-12-15 21:46:16 (68.7 MB/s) - ‘ptb.zip’ saved [1958845/1958845]\n","\n","Archive:  ptb.zip\n","  inflating: ptb.train.txt           \n","  inflating: ptb.valid.txt           \n","  inflating: ptb.test.txt            \n"]}],"source":["import os.path\r\n","if not(os.path.isfile('ptb.zip')):\r\n","  !wget -O ptb.zip https://www.dropbox.com/s/bs9ztyq27sxa3l7/ptb.zip?dl=0\r\n","!unzip -o ptb.zip"]},{"cell_type":"markdown","metadata":{"id":"Qi7eFsm2Bv2L"},"source":["## Load Tensorboard:"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3890,"status":"ok","timestamp":1608068777041,"user":{"displayName":"Artem Mironov","photoUrl":"","userId":"07007599338275803513"},"user_tz":-120},"id":"TJ8xzUTT826j"},"outputs":[],"source":["%load_ext tensorboard\r\n","logdir = \"tensorboard_logs\""]},{"cell_type":"markdown","metadata":{"id":"YU2rE3nUxnhN"},"source":["## Define Hyperparameters:"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3889,"status":"ok","timestamp":1608068777042,"user":{"displayName":"Artem Mironov","photoUrl":"","userId":"07007599338275803513"},"user_tz":-120},"id":"mvX6ZnZqxm-z"},"outputs":[],"source":["batch_size = 20\r\n","seq_len = 25\r\n","clip_norm = 5\r\n","learning_rate = 1.\r\n","momentum = 0.8\r\n","decay = 0.98\r\n","epochs = 70\r\n","epochs_no_decay = 15\r\n","drop_out_rate = 0.4\r\n","hidden_size = 200"]},{"cell_type":"markdown","metadata":{"id":"2ybPw3GS_3mq"},"source":["## Define Custom Method for Features and Labels Creation from sequence:"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3888,"status":"ok","timestamp":1608068777042,"user":{"displayName":"Artem Mironov","photoUrl":"","userId":"07007599338275803513"},"user_tz":-120},"id":"K4rHtY784thO"},"outputs":[],"source":["# Based on Git Repo by @tmatha (2018)\r\n","# Modified based on MIT License\r\n","def features_labels(data_array, batch_size, seq_len, batch_first=True):  \r\n","  if len(data_array.shape) != 1:\r\n","    raise ValueError('Expected 1-d data array, '\r\n","                     'instead data array shape is {} '.format(data_array.shape))\r\n","  \r\n","  def fold(used_array):\r\n","    shaped_array=np.reshape(used_array,(batch_size,seq_len*steps),order='C')    \r\n","    if batch_first:\r\n","      return np.concatenate(np.split(shaped_array,steps,axis=1),axis=0)\r\n","    else:\r\n","      return np.transpose(shaped_array)\r\n","\r\n","  steps = (data_array.shape[0]-1)//(batch_size*seq_len)\r\n","  used = batch_size * seq_len * steps  \r\n","  features = fold(data_array[:used])\r\n","  labels   = fold(data_array[1:used+1])\r\n","  \r\n","  Data = collections.namedtuple('Data',['features','labels'])\r\n","  return Data(features=features,labels=labels), steps"]},{"cell_type":"markdown","metadata":{"id":"-HDaq6iAG8nv"},"source":["## Create Sequences:"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3882,"status":"ok","timestamp":1608068777043,"user":{"displayName":"Artem Mironov","photoUrl":"","userId":"07007599338275803513"},"user_tz":-120},"id":"cq_Ly6_sdMBz","outputId":"bc8e0165-e83a-4834-af60-ffb33db29ae1"},"outputs":[{"name":"stdout","output_type":"stream","text":["First ten in train sequence: ['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec']\n","Size of train sequence: 929589\n","First ten in valid sequence: ['consumers', 'may', 'want', 'to', 'move', 'their', 'telephones', 'a', 'little', 'closer']\n","Size of valid sequence: 73760\n","First ten in test sequence: ['no', 'it', 'was', \"n't\", 'black', 'monday', '\u003ceos\u003e', 'but', 'while', 'the']\n","Size of test sequence: 82430\n"]}],"source":["seq = {'train': [], 'valid': [], 'test': []}\r\n","for seq_type in seq.keys():\r\n","  with open('ptb.'+ seq_type + '.txt','r') as cur_file:\r\n","      cur_seq = cur_file.read().replace('\\n','\u003ceos\u003e').split(' ')\r\n","  cur_seq = list(filter(None, cur_seq))\r\n","  print(f'First ten in {seq_type} sequence: {cur_seq[:10]}')\r\n","  print(f'Size of {seq_type} sequence: {len(cur_seq)}')\r\n","  seq[seq_type] = cur_seq"]},{"cell_type":"markdown","metadata":{"id":"WZBSBIKs0TY4"},"source":["## Create the Vocabulary and Datasets:"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3877,"status":"ok","timestamp":1608068777043,"user":{"displayName":"Artem Mironov","photoUrl":"","userId":"07007599338275803513"},"user_tz":-120},"id":"LQslNluD0PoO","outputId":"70ae39d9-f34e-4ea2-d9de-56130da934e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train vocabulary length: 10000\n","Valid vocabulary length: 6022\n","Test vocabulary length: 6049\n"]}],"source":["vocab_train = set(seq['train'])\r\n","vocab_valid = set(seq['valid'])\r\n","vocab_test  = set(seq['test'])\r\n","\r\n","assert vocab_valid.issubset(vocab_train)\r\n","assert vocab_test.issubset(vocab_train)\r\n","print(f'Train vocabulary length: {len(vocab_train)}')\r\n","print(f'Valid vocabulary length: {len(vocab_valid)}')\r\n","print(f'Test vocabulary length: {len(vocab_test)}')\r\n","\r\n","vocab_train = sorted(vocab_train)\r\n","word2id = {w:i for i,w in enumerate(vocab_train)}\r\n","id2word = {i:w for i,w in enumerate(vocab_train)}"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4327,"status":"ok","timestamp":1608068777499,"user":{"displayName":"Artem Mironov","photoUrl":"","userId":"07007599338275803513"},"user_tz":-120},"id":"gjZBTJJ_4Ij5","outputId":"25881dc8-85c2-4c9d-b02c-c8266c97d334"},"outputs":[{"name":"stdout","output_type":"stream","text":["Steps Train: 1859\n","Steps Validation: 147\n","Steps Test: 164\n"]}],"source":["ids_train = np.array([word2id[word] for word in seq['train']], copy=False, order='C')\r\n","ids_valid = np.array([word2id[word] for word in seq['valid']], copy=False, order='C')\r\n","ids_test  = np.array([word2id[word] for word in seq['test']], copy=False, order='C')\r\n","\r\n","data_train, steps_train = features_labels(\r\n","    ids_train, batch_size, seq_len, batch_first=True)\r\n","data_valid, steps_valid = features_labels(\r\n","    ids_valid, batch_size, seq_len, batch_first=True)\r\n","data_test, steps_test = features_labels(\r\n","    ids_test, batch_size, seq_len, batch_first=True)\r\n","\r\n","print(f'Steps Train: {steps_train}')\r\n","print(f'Steps Validation: {steps_valid}')\r\n","print(f'Steps Test: {steps_test}')"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":9685,"status":"ok","timestamp":1608068782858,"user":{"displayName":"Artem Mironov","photoUrl":"","userId":"07007599338275803513"},"user_tz":-120},"id":"kl7B0mTR5NMh"},"outputs":[],"source":["dataset_train = tf.data.Dataset.from_tensor_slices(data_train).batch(batch_size,\r\n","    drop_remainder=True)\r\n","dataset_valid = tf.data.Dataset.from_tensor_slices(data_valid).batch(batch_size,\r\n","    drop_remainder=True)\r\n","dataset_test = tf.data.Dataset.from_tensor_slices(data_test).batch(batch_size,\r\n","    drop_remainder=True)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9681,"status":"ok","timestamp":1608068782859,"user":{"displayName":"Artem Mironov","photoUrl":"","userId":"07007599338275803513"},"user_tz":-120},"id":"2nxkJ8de1B_j","outputId":"81b562b7-fd2a-407f-e9f5-2bbfba5580f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of input: (20, 25)\n","Shape of target: (20, 25)\n","aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \u003ceos\u003e\n","banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \u003ceos\u003e pierre\n","\u003cunk\u003e on airline acquisitions that would so load a carrier up with debt that it would \u003cunk\u003e safety or a carrier 's ability to compete\n","on airline acquisitions that would so load a carrier up with debt that it would \u003cunk\u003e safety or a carrier 's ability to compete rep.\n"]}],"source":["# Check the dataset's shapes, input and target:\r\n","for input_example_batch, target_example_batch in dataset_train.take(1):\r\n","    print(f'Shape of input: {input_example_batch.shape}')\r\n","    print(f'Shape of target: {target_example_batch.shape}')\r\n","    \r\n","    print(' '.join([id2word[id] for id in input_example_batch[0].numpy()]))\r\n","    print(' '.join([id2word[id] for id in target_example_batch[0].numpy()]))\r\n","    print(' '.join([id2word[id] for id in input_example_batch[1].numpy()]))\r\n","    print(' '.join([id2word[id] for id in target_example_batch[1].numpy()]))"]},{"cell_type":"markdown","metadata":{"id":"JFLAerdlxQ7J"},"source":["## Define LSTM Based Sequential Model Class:\r\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":9680,"status":"ok","timestamp":1608068782860,"user":{"displayName":"Artem Mironov","photoUrl":"","userId":"07007599338275803513"},"user_tz":-120},"id":"zP9T3h3V8Yoa"},"outputs":[],"source":["class LSTM_Based(Sequential):\r\n","  def __init__(self, name, vocabulary_size, hidden_size, seq_len, Drop=None):\r\n","    super().__init__(name = name)\r\n","    self.add(Embedding(vocabulary_size, hidden_size, input_length=seq_len))\r\n","    if Drop:\r\n","      self.add(Dropout(drop_out_rate))\r\n","    self.add(LSTM(hidden_size, return_sequences=True))\r\n","    if Drop:\r\n","      self.add(Dropout(drop_out_rate))\r\n","    self.add(LSTM(hidden_size, return_sequences=True))\r\n","    if Drop:\r\n","      self.add(Dropout(drop_out_rate))\r\n","    self.add(Dense(vocabulary_size))"]},{"cell_type":"markdown","metadata":{"id":"feZ2FJXgxFW_"},"source":["## Define Custom Perplexity Metric, Learning Scheduler and Optimizer:"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":9678,"status":"ok","timestamp":1608068782860,"user":{"displayName":"Artem Mironov","photoUrl":"","userId":"07007599338275803513"},"user_tz":-120},"id":"m_qJmEXkPp9O"},"outputs":[],"source":["def perplexity(y_true, y_pred):\r\n","    cross_entropy = K.mean(K.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True))\r\n","    perplexity = K.exp(cross_entropy)\r\n","    return perplexity\r\n","\r\n","def lr_decay(epoch, lr):\r\n","  if epoch \u003c epochs_no_decay:\r\n","    return learning_rate\r\n","  else:\r\n","    return max(round(lr * decay, 2), 0.01)\r\n","\r\n","def get_callbacks(name):\r\n","  log_dir = logdir + \"/\" + name + \"/\" + datetime.datetime.now().strftime(\"%m%d-%H%M\")\r\n","  file_path = name + '.weights.hdf5'\r\n","  checkpointer = ModelCheckpoint(filepath=file_path, verbose = 1, save_best_only=True)\r\n","  return [\r\n","          LearningRateScheduler(lr_decay, verbose=1),\r\n","          TensorBoard(log_dir=log_dir, histogram_freq=1),\r\n","          checkpointer\r\n","  ]"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":9678,"status":"ok","timestamp":1608068782861,"user":{"displayName":"Artem Mironov","photoUrl":"","userId":"07007599338275803513"},"user_tz":-120},"id":"SEbQGBpwpYSz"},"outputs":[],"source":["def create_compile(model_def):\r\n","  if model_def['Type'] == 'LSTM':\r\n","    model = LSTM_Based(name = model_def['name'],\r\n","                       vocabulary_size = len(word2id),\r\n","                       hidden_size = model_def['hidden_size'],\r\n","                       seq_len = model_def['seq_len'],\r\n","                       Drop = model_def['Drop'])\r\n","  elif model_def['Type'] == 'GRU':\r\n","    model = GRU_Based(vocabulary_size = len(word2id),\r\n","                       hidden_size = model_def['hidden_size'],\r\n","                       seq_len = model_def['seq_len'],\r\n","                       Drop = model_def['Drop'])\r\n","    \r\n","  optimizer = tf.keras.optimizers.SGD(learning_rate=0.01,\r\n","                                      clipnorm=clip_norm,\r\n","                                      momentum=momentum)\r\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n","  model.compile(optimizer=optimizer, loss=loss, metrics=[perplexity])\r\n","  return model"]},{"cell_type":"markdown","metadata":{"id":"Mw_UvsB1ouJS"},"source":["## Train Models:"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":9675,"status":"ok","timestamp":1608068782861,"user":{"displayName":"Artem Mironov","photoUrl":"","userId":"07007599338275803513"},"user_tz":-120},"id":"CqhPu2CdotT0"},"outputs":[],"source":["model_defs = [\r\n","              {'name': 'LSTM_Drop', 'Type': 'LSTM', 'hidden_size': hidden_size, 'seq_len': seq_len, 'Drop': True},\r\n","              {'name': 'LSTM_No_Drop', 'Type': 'LSTM', 'hidden_size': hidden_size, 'seq_len': seq_len, 'Drop': False},\r\n","              ]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"cR9qMaDmpL3s"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"LSTM_Drop\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 25, 200)           2000000   \n","_________________________________________________________________\n","dropout (Dropout)            (None, 25, 200)           0         \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 25, 200)           320800    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 25, 200)           0         \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 25, 200)           320800    \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 25, 200)           0         \n","_________________________________________________________________\n","dense (Dense)                (None, 25, 10000)         2010000   \n","=================================================================\n","Total params: 4,651,600\n","Trainable params: 4,651,600\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","Epoch 00001: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 1/70\n","   1/1859 [..............................] - ETA: 0s - loss: 9.2103 - perplexity: 9999.9346WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n","Instructions for updating:\n","use `tf.profiler.experimental.stop` instead.\n","   2/1859 [..............................] - ETA: 1:34 - loss: 9.2043 - perplexity: 9939.8887WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0231s vs `on_train_batch_end` time: 0.0785s). Check your callbacks.\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 6.2372 - perplexity: 582.9976\n","Epoch 00001: val_loss improved from inf to 5.75885, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 6.2366 - perplexity: 582.5815 - val_loss: 5.7588 - val_perplexity: 321.2507\n","\n","Epoch 00002: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 2/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 5.6270 - perplexity: 284.2800\n","Epoch 00002: val_loss improved from 5.75885 to 5.41314, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 22s 12ms/step - loss: 5.6270 - perplexity: 284.3025 - val_loss: 5.4131 - val_perplexity: 227.6830\n","\n","Epoch 00003: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 3/70\n","1859/1859 [==============================] - ETA: 0s - loss: 5.3819 - perplexity: 221.7194\n","Epoch 00003: val_loss improved from 5.41314 to 5.25863, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 5.3819 - perplexity: 221.7194 - val_loss: 5.2586 - val_perplexity: 195.2294\n","\n","Epoch 00004: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 4/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 5.2398 - perplexity: 192.3322\n","Epoch 00004: val_loss improved from 5.25863 to 5.15556, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 5.2400 - perplexity: 192.3884 - val_loss: 5.1556 - val_perplexity: 176.1934\n","\n","Epoch 00005: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 5/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 5.1411 - perplexity: 174.3140\n","Epoch 00005: val_loss improved from 5.15556 to 5.08500, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 5.1415 - perplexity: 174.3829 - val_loss: 5.0850 - val_perplexity: 164.1692\n","\n","Epoch 00006: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 6/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 5.0707 - perplexity: 162.4295\n","Epoch 00006: val_loss improved from 5.08500 to 5.03281, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 22s 12ms/step - loss: 5.0710 - perplexity: 162.4719 - val_loss: 5.0328 - val_perplexity: 155.9846\n","\n","Epoch 00007: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 7/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 5.0117 - perplexity: 153.1738\n","Epoch 00007: val_loss improved from 5.03281 to 4.99621, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 22s 12ms/step - loss: 5.0120 - perplexity: 153.2294 - val_loss: 4.9962 - val_perplexity: 150.3894\n","\n","Epoch 00008: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 8/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.9638 - perplexity: 146.0453\n","Epoch 00008: val_loss improved from 4.99621 to 4.96653, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.9641 - perplexity: 146.0847 - val_loss: 4.9665 - val_perplexity: 146.0791\n","\n","Epoch 00009: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 9/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.9218 - perplexity: 140.0616\n","Epoch 00009: val_loss improved from 4.96653 to 4.93676, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.9220 - perplexity: 140.0934 - val_loss: 4.9368 - val_perplexity: 141.8410\n","\n","Epoch 00010: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 10/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.8852 - perplexity: 135.0048\n","Epoch 00010: val_loss improved from 4.93676 to 4.91410, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.8852 - perplexity: 135.0048 - val_loss: 4.9141 - val_perplexity: 138.6885\n","\n","Epoch 00011: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 11/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.8530 - perplexity: 130.7399\n","Epoch 00011: val_loss improved from 4.91410 to 4.89291, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.8530 - perplexity: 130.7399 - val_loss: 4.8929 - val_perplexity: 135.8613\n","\n","Epoch 00012: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 12/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 4.8240 - perplexity: 127.0450\n","Epoch 00012: val_loss improved from 4.89291 to 4.87845, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.8244 - perplexity: 127.0961 - val_loss: 4.8785 - val_perplexity: 133.9380\n","\n","Epoch 00013: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 13/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.7994 - perplexity: 123.9377\n","Epoch 00013: val_loss improved from 4.87845 to 4.86350, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.7994 - perplexity: 123.9377 - val_loss: 4.8635 - val_perplexity: 131.9803\n","\n","Epoch 00014: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 14/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 4.7750 - perplexity: 120.9332\n","Epoch 00014: val_loss improved from 4.86350 to 4.85215, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.7753 - perplexity: 120.9720 - val_loss: 4.8522 - val_perplexity: 130.4606\n","\n","Epoch 00015: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 15/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.7543 - perplexity: 118.4551\n","Epoch 00015: val_loss improved from 4.85215 to 4.83815, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 22s 12ms/step - loss: 4.7543 - perplexity: 118.4617 - val_loss: 4.8381 - val_perplexity: 128.7666\n","\n","Epoch 00016: LearningRateScheduler reducing learning rate to 0.98.\n","Epoch 16/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.7336 - perplexity: 116.0078\n","Epoch 00016: val_loss improved from 4.83815 to 4.82643, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 22s 12ms/step - loss: 4.7336 - perplexity: 116.0078 - val_loss: 4.8264 - val_perplexity: 127.2341\n","\n","Epoch 00017: LearningRateScheduler reducing learning rate to 0.96.\n","Epoch 17/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.7119 - perplexity: 113.5568\n","Epoch 00017: val_loss improved from 4.82643 to 4.82269, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.7122 - perplexity: 113.5938 - val_loss: 4.8227 - val_perplexity: 126.7515\n","\n","Epoch 00018: LearningRateScheduler reducing learning rate to 0.94.\n","Epoch 18/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.6945 - perplexity: 111.5710\n","Epoch 00018: val_loss improved from 4.82269 to 4.81076, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.6945 - perplexity: 111.5752 - val_loss: 4.8108 - val_perplexity: 125.3317\n","\n","Epoch 00019: LearningRateScheduler reducing learning rate to 0.92.\n","Epoch 19/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.6773 - perplexity: 109.6599\n","Epoch 00019: val_loss improved from 4.81076 to 4.80261, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.6775 - perplexity: 109.6843 - val_loss: 4.8026 - val_perplexity: 124.3300\n","\n","Epoch 00020: LearningRateScheduler reducing learning rate to 0.9.\n","Epoch 20/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.6629 - perplexity: 108.0864\n","Epoch 00020: val_loss improved from 4.80261 to 4.79774, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.6631 - perplexity: 108.1120 - val_loss: 4.7977 - val_perplexity: 123.7869\n","\n","Epoch 00021: LearningRateScheduler reducing learning rate to 0.88.\n","Epoch 21/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.6492 - perplexity: 106.5901\n","Epoch 00021: val_loss improved from 4.79774 to 4.78876, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 13ms/step - loss: 4.6492 - perplexity: 106.5901 - val_loss: 4.7888 - val_perplexity: 122.6334\n","\n","Epoch 00022: LearningRateScheduler reducing learning rate to 0.86.\n","Epoch 22/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 4.6334 - perplexity: 104.9311\n","Epoch 00022: val_loss improved from 4.78876 to 4.78277, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.6338 - perplexity: 104.9738 - val_loss: 4.7828 - val_perplexity: 121.8630\n","\n","Epoch 00023: LearningRateScheduler reducing learning rate to 0.84.\n","Epoch 23/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.6191 - perplexity: 103.4429\n","Epoch 00023: val_loss improved from 4.78277 to 4.77819, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 13ms/step - loss: 4.6192 - perplexity: 103.4504 - val_loss: 4.7782 - val_perplexity: 121.3746\n","\n","Epoch 00024: LearningRateScheduler reducing learning rate to 0.82.\n","Epoch 24/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.6099 - perplexity: 102.4901\n","Epoch 00024: val_loss improved from 4.77819 to 4.77162, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 13ms/step - loss: 4.6099 - perplexity: 102.4901 - val_loss: 4.7716 - val_perplexity: 120.5708\n","\n","Epoch 00025: LearningRateScheduler reducing learning rate to 0.8.\n","Epoch 25/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.5975 - perplexity: 101.2548\n","Epoch 00025: val_loss improved from 4.77162 to 4.76678, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.5975 - perplexity: 101.2548 - val_loss: 4.7668 - val_perplexity: 119.9805\n","\n","Epoch 00026: LearningRateScheduler reducing learning rate to 0.78.\n","Epoch 26/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.5844 - perplexity: 99.9149\n","Epoch 00026: val_loss did not improve from 4.76678\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.5844 - perplexity: 99.9149 - val_loss: 4.7674 - val_perplexity: 120.0866\n","\n","Epoch 00027: LearningRateScheduler reducing learning rate to 0.76.\n","Epoch 27/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.5717 - perplexity: 98.6264\n","Epoch 00027: val_loss improved from 4.76678 to 4.76065, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.5719 - perplexity: 98.6483 - val_loss: 4.7606 - val_perplexity: 119.2812\n","\n","Epoch 00028: LearningRateScheduler reducing learning rate to 0.74.\n","Epoch 28/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.5654 - perplexity: 98.0235\n","Epoch 00028: val_loss improved from 4.76065 to 4.75641, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 13ms/step - loss: 4.5654 - perplexity: 98.0285 - val_loss: 4.7564 - val_perplexity: 118.7688\n","\n","Epoch 00029: LearningRateScheduler reducing learning rate to 0.73.\n","Epoch 29/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 4.5537 - perplexity: 96.8632\n","Epoch 00029: val_loss improved from 4.75641 to 4.75425, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.5541 - perplexity: 96.9132 - val_loss: 4.7543 - val_perplexity: 118.5193\n","\n","Epoch 00030: LearningRateScheduler reducing learning rate to 0.72.\n","Epoch 30/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 4.5453 - perplexity: 96.0487\n","Epoch 00030: val_loss improved from 4.75425 to 4.75006, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.5457 - perplexity: 96.0783 - val_loss: 4.7501 - val_perplexity: 118.0725\n","\n","Epoch 00031: LearningRateScheduler reducing learning rate to 0.71.\n","Epoch 31/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 4.5366 - perplexity: 95.2091\n","Epoch 00031: val_loss improved from 4.75006 to 4.74701, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.5370 - perplexity: 95.2465 - val_loss: 4.7470 - val_perplexity: 117.7218\n","\n","Epoch 00032: LearningRateScheduler reducing learning rate to 0.7.\n","Epoch 32/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.5310 - perplexity: 94.6678\n","Epoch 00032: val_loss improved from 4.74701 to 4.74317, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.5310 - perplexity: 94.6708 - val_loss: 4.7432 - val_perplexity: 117.3023\n","\n","Epoch 00033: LearningRateScheduler reducing learning rate to 0.69.\n","Epoch 33/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.5223 - perplexity: 93.8808\n","Epoch 00033: val_loss improved from 4.74317 to 4.74189, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.5224 - perplexity: 93.8857 - val_loss: 4.7419 - val_perplexity: 117.1340\n","\n","Epoch 00034: LearningRateScheduler reducing learning rate to 0.68.\n","Epoch 34/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 4.5143 - perplexity: 93.1043\n","Epoch 00034: val_loss improved from 4.74189 to 4.74043, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 13ms/step - loss: 4.5147 - perplexity: 93.1430 - val_loss: 4.7404 - val_perplexity: 116.9683\n","\n","Epoch 00035: LearningRateScheduler reducing learning rate to 0.67.\n","Epoch 35/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.5088 - perplexity: 92.5871\n","Epoch 00035: val_loss improved from 4.74043 to 4.73727, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.5089 - perplexity: 92.5921 - val_loss: 4.7373 - val_perplexity: 116.6322\n","\n","Epoch 00036: LearningRateScheduler reducing learning rate to 0.66.\n","Epoch 36/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 4.5010 - perplexity: 91.8784\n","Epoch 00036: val_loss did not improve from 4.73727\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.5013 - perplexity: 91.9110 - val_loss: 4.7387 - val_perplexity: 116.8268\n","\n","Epoch 00037: LearningRateScheduler reducing learning rate to 0.65.\n","Epoch 37/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.4966 - perplexity: 91.4933\n","Epoch 00037: val_loss did not improve from 4.73727\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4966 - perplexity: 91.4933 - val_loss: 4.7379 - val_perplexity: 116.7299\n","\n","Epoch 00038: LearningRateScheduler reducing learning rate to 0.64.\n","Epoch 38/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 4.4880 - perplexity: 90.6979\n","Epoch 00038: val_loss improved from 4.73727 to 4.73635, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4884 - perplexity: 90.7314 - val_loss: 4.7364 - val_perplexity: 116.5519\n","\n","Epoch 00039: LearningRateScheduler reducing learning rate to 0.63.\n","Epoch 39/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.4834 - perplexity: 90.2960\n","Epoch 00039: val_loss did not improve from 4.73635\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4834 - perplexity: 90.3012 - val_loss: 4.7364 - val_perplexity: 116.6043\n","\n","Epoch 00040: LearningRateScheduler reducing learning rate to 0.62.\n","Epoch 40/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.4754 - perplexity: 89.5565\n","Epoch 00040: val_loss improved from 4.73635 to 4.73187, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4756 - perplexity: 89.5824 - val_loss: 4.7319 - val_perplexity: 116.0442\n","\n","Epoch 00041: LearningRateScheduler reducing learning rate to 0.61.\n","Epoch 41/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 4.4717 - perplexity: 89.2149\n","Epoch 00041: val_loss did not improve from 4.73187\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4721 - perplexity: 89.2549 - val_loss: 4.7321 - val_perplexity: 116.0667\n","\n","Epoch 00042: LearningRateScheduler reducing learning rate to 0.6.\n","Epoch 42/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.4639 - perplexity: 88.5146\n","Epoch 00042: val_loss improved from 4.73187 to 4.73022, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4639 - perplexity: 88.5146 - val_loss: 4.7302 - val_perplexity: 115.8325\n","\n","Epoch 00043: LearningRateScheduler reducing learning rate to 0.59.\n","Epoch 43/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.4596 - perplexity: 88.1196\n","Epoch 00043: val_loss improved from 4.73022 to 4.72783, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 13ms/step - loss: 4.4598 - perplexity: 88.1448 - val_loss: 4.7278 - val_perplexity: 115.5803\n","\n","Epoch 00044: LearningRateScheduler reducing learning rate to 0.58.\n","Epoch 44/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.4536 - perplexity: 87.6018\n","Epoch 00044: val_loss improved from 4.72783 to 4.72596, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4538 - perplexity: 87.6225 - val_loss: 4.7260 - val_perplexity: 115.3859\n","\n","Epoch 00045: LearningRateScheduler reducing learning rate to 0.57.\n","Epoch 45/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.4487 - perplexity: 87.1966\n","Epoch 00045: val_loss improved from 4.72596 to 4.72528, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4487 - perplexity: 87.1989 - val_loss: 4.7253 - val_perplexity: 115.2937\n","\n","Epoch 00046: LearningRateScheduler reducing learning rate to 0.56.\n","Epoch 46/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.4438 - perplexity: 86.7509\n","Epoch 00046: val_loss improved from 4.72528 to 4.72291, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4439 - perplexity: 86.7553 - val_loss: 4.7229 - val_perplexity: 115.0449\n","\n","Epoch 00047: LearningRateScheduler reducing learning rate to 0.55.\n","Epoch 47/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.4377 - perplexity: 86.2053\n","Epoch 00047: val_loss improved from 4.72291 to 4.72089, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4377 - perplexity: 86.2088 - val_loss: 4.7209 - val_perplexity: 114.8091\n","\n","Epoch 00048: LearningRateScheduler reducing learning rate to 0.54.\n","Epoch 48/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.4327 - perplexity: 85.7938\n","Epoch 00048: val_loss improved from 4.72089 to 4.72088, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4329 - perplexity: 85.8137 - val_loss: 4.7209 - val_perplexity: 114.8162\n","\n","Epoch 00049: LearningRateScheduler reducing learning rate to 0.53.\n","Epoch 49/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 4.4288 - perplexity: 85.4424\n","Epoch 00049: val_loss improved from 4.72088 to 4.72031, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4292 - perplexity: 85.4768 - val_loss: 4.7203 - val_perplexity: 114.7700\n","\n","Epoch 00050: LearningRateScheduler reducing learning rate to 0.52.\n","Epoch 50/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.4234 - perplexity: 84.9782\n","Epoch 00050: val_loss improved from 4.72031 to 4.71871, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4236 - perplexity: 84.9988 - val_loss: 4.7187 - val_perplexity: 114.5892\n","\n","Epoch 00051: LearningRateScheduler reducing learning rate to 0.51.\n","Epoch 51/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 4.4211 - perplexity: 84.7734\n","Epoch 00051: val_loss did not improve from 4.71871\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4215 - perplexity: 84.8081 - val_loss: 4.7193 - val_perplexity: 114.6531\n","\n","Epoch 00052: LearningRateScheduler reducing learning rate to 0.5.\n","Epoch 52/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.4147 - perplexity: 84.2626\n","Epoch 00052: val_loss improved from 4.71871 to 4.71654, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4149 - perplexity: 84.2831 - val_loss: 4.7165 - val_perplexity: 114.3177\n","\n","Epoch 00053: LearningRateScheduler reducing learning rate to 0.49.\n","Epoch 53/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 4.4114 - perplexity: 84.0008\n","Epoch 00053: val_loss improved from 4.71654 to 4.71627, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4118 - perplexity: 84.0358 - val_loss: 4.7163 - val_perplexity: 114.3382\n","\n","Epoch 00054: LearningRateScheduler reducing learning rate to 0.48.\n","Epoch 54/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.4067 - perplexity: 83.5840\n","Epoch 00054: val_loss improved from 4.71627 to 4.71577, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4069 - perplexity: 83.6020 - val_loss: 4.7158 - val_perplexity: 114.3025\n","\n","Epoch 00055: LearningRateScheduler reducing learning rate to 0.47.\n","Epoch 55/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.4016 - perplexity: 83.1309\n","Epoch 00055: val_loss improved from 4.71577 to 4.71442, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.4016 - perplexity: 83.1309 - val_loss: 4.7144 - val_perplexity: 114.1187\n","\n","Epoch 00056: LearningRateScheduler reducing learning rate to 0.46.\n","Epoch 56/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.3976 - perplexity: 82.8149\n","Epoch 00056: val_loss improved from 4.71442 to 4.71428, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.3976 - perplexity: 82.8149 - val_loss: 4.7143 - val_perplexity: 114.1264\n","\n","Epoch 00057: LearningRateScheduler reducing learning rate to 0.45.\n","Epoch 57/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 4.3949 - perplexity: 82.5876\n","Epoch 00057: val_loss improved from 4.71428 to 4.71309, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.3952 - perplexity: 82.6123 - val_loss: 4.7131 - val_perplexity: 113.9896\n","\n","Epoch 00058: LearningRateScheduler reducing learning rate to 0.44.\n","Epoch 58/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.3894 - perplexity: 82.1485\n","Epoch 00058: val_loss improved from 4.71309 to 4.71034, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.3894 - perplexity: 82.1524 - val_loss: 4.7103 - val_perplexity: 113.6809\n","\n","Epoch 00059: LearningRateScheduler reducing learning rate to 0.43.\n","Epoch 59/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.3859 - perplexity: 81.8516\n","Epoch 00059: val_loss improved from 4.71034 to 4.70978, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.3859 - perplexity: 81.8571 - val_loss: 4.7098 - val_perplexity: 113.6227\n","\n","Epoch 00060: LearningRateScheduler reducing learning rate to 0.42.\n","Epoch 60/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.3820 - perplexity: 81.5367\n","Epoch 00060: val_loss did not improve from 4.70978\n","1859/1859 [==============================] - 23s 13ms/step - loss: 4.3820 - perplexity: 81.5367 - val_loss: 4.7100 - val_perplexity: 113.6399\n","\n","Epoch 00061: LearningRateScheduler reducing learning rate to 0.41.\n","Epoch 61/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 4.3759 - perplexity: 81.0329\n","Epoch 00061: val_loss did not improve from 4.70978\n","1859/1859 [==============================] - 23s 13ms/step - loss: 4.3763 - perplexity: 81.0623 - val_loss: 4.7125 - val_perplexity: 113.9258\n","\n","Epoch 00062: LearningRateScheduler reducing learning rate to 0.4.\n","Epoch 62/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.3724 - perplexity: 80.7627\n","Epoch 00062: val_loss improved from 4.70978 to 4.70936, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.3725 - perplexity: 80.7660 - val_loss: 4.7094 - val_perplexity: 113.6237\n","\n","Epoch 00063: LearningRateScheduler reducing learning rate to 0.39.\n","Epoch 63/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.3698 - perplexity: 80.5183\n","Epoch 00063: val_loss did not improve from 4.70936\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.3698 - perplexity: 80.5219 - val_loss: 4.7094 - val_perplexity: 113.6054\n","\n","Epoch 00064: LearningRateScheduler reducing learning rate to 0.38.\n","Epoch 64/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.3657 - perplexity: 80.2386\n","Epoch 00064: val_loss improved from 4.70936 to 4.70901, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.3658 - perplexity: 80.2450 - val_loss: 4.7090 - val_perplexity: 113.5638\n","\n","Epoch 00065: LearningRateScheduler reducing learning rate to 0.37.\n","Epoch 65/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.3608 - perplexity: 79.7978\n","Epoch 00065: val_loss improved from 4.70901 to 4.70797, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.3610 - perplexity: 79.8145 - val_loss: 4.7080 - val_perplexity: 113.4528\n","\n","Epoch 00066: LearningRateScheduler reducing learning rate to 0.36.\n","Epoch 66/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 4.3569 - perplexity: 79.5166\n","Epoch 00066: val_loss improved from 4.70797 to 4.70764, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.3572 - perplexity: 79.5406 - val_loss: 4.7076 - val_perplexity: 113.3984\n","\n","Epoch 00067: LearningRateScheduler reducing learning rate to 0.35.\n","Epoch 67/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.3552 - perplexity: 79.3880\n","Epoch 00067: val_loss improved from 4.70764 to 4.70641, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.3553 - perplexity: 79.4045 - val_loss: 4.7064 - val_perplexity: 113.2662\n","\n","Epoch 00068: LearningRateScheduler reducing learning rate to 0.34.\n","Epoch 68/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.3509 - perplexity: 79.0255\n","Epoch 00068: val_loss improved from 4.70641 to 4.70632, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.3509 - perplexity: 79.0255 - val_loss: 4.7063 - val_perplexity: 113.2779\n","\n","Epoch 00069: LearningRateScheduler reducing learning rate to 0.33.\n","Epoch 69/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 4.3468 - perplexity: 78.7211\n","Epoch 00069: val_loss improved from 4.70632 to 4.70528, saving model to LSTM_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.3473 - perplexity: 78.7566 - val_loss: 4.7053 - val_perplexity: 113.1366\n","\n","Epoch 00070: LearningRateScheduler reducing learning rate to 0.32.\n","Epoch 70/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.3430 - perplexity: 78.4192\n","Epoch 00070: val_loss did not improve from 4.70528\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.3433 - perplexity: 78.4363 - val_loss: 4.7068 - val_perplexity: 113.3525\n","Model: \"LSTM_No_Drop\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 25, 200)           2000000   \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 25, 200)           320800    \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 25, 200)           320800    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 25, 10000)         2010000   \n","=================================================================\n","Total params: 4,651,600\n","Trainable params: 4,651,600\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","Epoch 00001: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 1/70\n","   2/1859 [..............................] - ETA: 2:18 - loss: 9.2044 - perplexity: 9940.8906WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0234s vs `on_train_batch_end` time: 0.1249s). Check your callbacks.\n","1859/1859 [==============================] - ETA: 0s - loss: 6.1194 - perplexity: 532.3164\n","Epoch 00001: val_loss improved from inf to 5.68832, saving model to LSTM_No_Drop.weights.hdf5\n","1859/1859 [==============================] - 25s 13ms/step - loss: 6.1194 - perplexity: 532.3164 - val_loss: 5.6883 - val_perplexity: 299.6907\n","\n","Epoch 00002: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 2/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 5.4402 - perplexity: 236.4657\n","Epoch 00002: val_loss improved from 5.68832 to 5.36835, saving model to LSTM_No_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 5.4402 - perplexity: 236.4722 - val_loss: 5.3683 - val_perplexity: 217.9632\n","\n","Epoch 00003: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 3/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 5.1612 - perplexity: 178.3510\n","Epoch 00003: val_loss improved from 5.36835 to 5.21316, saving model to LSTM_No_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 13ms/step - loss: 5.1614 - perplexity: 178.3938 - val_loss: 5.2132 - val_perplexity: 186.7283\n","\n","Epoch 00004: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 4/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.9822 - perplexity: 149.0947\n","Epoch 00004: val_loss improved from 5.21316 to 5.11672, saving model to LSTM_No_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.9824 - perplexity: 149.1277 - val_loss: 5.1167 - val_perplexity: 169.6425\n","\n","Epoch 00005: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 5/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.8457 - perplexity: 130.0971\n","Epoch 00005: val_loss improved from 5.11672 to 5.05992, saving model to LSTM_No_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.8457 - perplexity: 130.0971 - val_loss: 5.0599 - val_perplexity: 160.3749\n","\n","Epoch 00006: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 6/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.7331 - perplexity: 116.2608\n","Epoch 00006: val_loss improved from 5.05992 to 5.02490, saving model to LSTM_No_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 13ms/step - loss: 4.7331 - perplexity: 116.2608 - val_loss: 5.0249 - val_perplexity: 155.0650\n","\n","Epoch 00007: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 7/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.6360 - perplexity: 105.5110\n","Epoch 00007: val_loss improved from 5.02490 to 5.00383, saving model to LSTM_No_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 13ms/step - loss: 4.6360 - perplexity: 105.5133 - val_loss: 5.0038 - val_perplexity: 151.9255\n","\n","Epoch 00008: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 8/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 4.5501 - perplexity: 96.8315\n","Epoch 00008: val_loss improved from 5.00383 to 4.99418, saving model to LSTM_No_Drop.weights.hdf5\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.5505 - perplexity: 96.8629 - val_loss: 4.9942 - val_perplexity: 150.6492\n","\n","Epoch 00009: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 9/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 4.4740 - perplexity: 89.7153\n","Epoch 00009: val_loss improved from 4.99418 to 4.99382, saving model to LSTM_No_Drop.weights.hdf5\n","1859/1859 [==============================] - 23s 13ms/step - loss: 4.4744 - perplexity: 89.7441 - val_loss: 4.9938 - val_perplexity: 150.6538\n","\n","Epoch 00010: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 10/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 4.4055 - perplexity: 83.7587\n","Epoch 00010: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 23s 13ms/step - loss: 4.4058 - perplexity: 83.7880 - val_loss: 4.9978 - val_perplexity: 151.2850\n","\n","Epoch 00011: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 11/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.3451 - perplexity: 78.8149\n","Epoch 00011: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 23s 13ms/step - loss: 4.3451 - perplexity: 78.8165 - val_loss: 5.0072 - val_perplexity: 152.8200\n","\n","Epoch 00012: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 12/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 4.2894 - perplexity: 74.5310\n","Epoch 00012: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 23s 13ms/step - loss: 4.2895 - perplexity: 74.5336 - val_loss: 5.0170 - val_perplexity: 154.5096\n","\n","Epoch 00013: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 13/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.2402 - perplexity: 70.9313\n","Epoch 00013: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 23s 13ms/step - loss: 4.2402 - perplexity: 70.9313 - val_loss: 5.0287 - val_perplexity: 156.3436\n","\n","Epoch 00014: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 14/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.1956 - perplexity: 67.8237\n","Epoch 00014: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.1956 - perplexity: 67.8237 - val_loss: 5.0400 - val_perplexity: 158.0203\n","\n","Epoch 00015: LearningRateScheduler reducing learning rate to 1.0.\n","Epoch 15/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.1557 - perplexity: 65.1484\n","Epoch 00015: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 23s 12ms/step - loss: 4.1559 - perplexity: 65.1665 - val_loss: 5.0505 - val_perplexity: 159.8140\n","\n","Epoch 00016: LearningRateScheduler reducing learning rate to 0.98.\n","Epoch 16/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 4.1153 - perplexity: 62.5529\n","Epoch 00016: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 27s 14ms/step - loss: 4.1156 - perplexity: 62.5691 - val_loss: 5.0579 - val_perplexity: 161.1975\n","\n","Epoch 00017: LearningRateScheduler reducing learning rate to 0.96.\n","Epoch 17/70\n","1859/1859 [==============================] - ETA: 0s - loss: 4.0739 - perplexity: 60.0055\n","Epoch 00017: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.0739 - perplexity: 60.0055 - val_loss: 5.0705 - val_perplexity: 163.3734\n","\n","Epoch 00018: LearningRateScheduler reducing learning rate to 0.94.\n","Epoch 18/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 4.0348 - perplexity: 57.6930\n","Epoch 00018: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 4.0352 - perplexity: 57.7179 - val_loss: 5.0855 - val_perplexity: 165.7671\n","\n","Epoch 00019: LearningRateScheduler reducing learning rate to 0.92.\n","Epoch 19/70\n","1859/1859 [==============================] - ETA: 0s - loss: 3.9985 - perplexity: 55.6137\n","Epoch 00019: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.9985 - perplexity: 55.6137 - val_loss: 5.1054 - val_perplexity: 168.9840\n","\n","Epoch 00020: LearningRateScheduler reducing learning rate to 0.9.\n","Epoch 20/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 3.9640 - perplexity: 53.7211\n","Epoch 00020: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.9640 - perplexity: 53.7230 - val_loss: 5.1151 - val_perplexity: 170.6009\n","\n","Epoch 00021: LearningRateScheduler reducing learning rate to 0.88.\n","Epoch 21/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 3.9281 - perplexity: 51.8148\n","Epoch 00021: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.9284 - perplexity: 51.8305 - val_loss: 5.1310 - val_perplexity: 173.5803\n","\n","Epoch 00022: LearningRateScheduler reducing learning rate to 0.86.\n","Epoch 22/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 3.8971 - perplexity: 50.2001\n","Epoch 00022: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.8973 - perplexity: 50.2122 - val_loss: 5.1348 - val_perplexity: 174.3265\n","\n","Epoch 00023: LearningRateScheduler reducing learning rate to 0.84.\n","Epoch 23/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 3.8638 - perplexity: 48.5524\n","Epoch 00023: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.8641 - perplexity: 48.5658 - val_loss: 5.1514 - val_perplexity: 177.3582\n","\n","Epoch 00024: LearningRateScheduler reducing learning rate to 0.82.\n","Epoch 24/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 3.8317 - perplexity: 46.9920\n","Epoch 00024: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.8321 - perplexity: 47.0082 - val_loss: 5.1744 - val_perplexity: 181.3972\n","\n","Epoch 00025: LearningRateScheduler reducing learning rate to 0.8.\n","Epoch 25/70\n","1859/1859 [==============================] - ETA: 0s - loss: 3.8040 - perplexity: 45.7125\n","Epoch 00025: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.8040 - perplexity: 45.7125 - val_loss: 5.1823 - val_perplexity: 182.8928\n","\n","Epoch 00026: LearningRateScheduler reducing learning rate to 0.78.\n","Epoch 26/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 3.7712 - perplexity: 44.2224\n","Epoch 00026: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.7716 - perplexity: 44.2397 - val_loss: 5.1918 - val_perplexity: 184.7681\n","\n","Epoch 00027: LearningRateScheduler reducing learning rate to 0.76.\n","Epoch 27/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 3.7405 - perplexity: 42.8875\n","Epoch 00027: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.7406 - perplexity: 42.8883 - val_loss: 5.2052 - val_perplexity: 187.4427\n","\n","Epoch 00028: LearningRateScheduler reducing learning rate to 0.74.\n","Epoch 28/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 3.7113 - perplexity: 41.6332\n","Epoch 00028: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.7116 - perplexity: 41.6471 - val_loss: 5.2224 - val_perplexity: 190.8315\n","\n","Epoch 00029: LearningRateScheduler reducing learning rate to 0.73.\n","Epoch 29/70\n","1859/1859 [==============================] - ETA: 0s - loss: 3.6860 - perplexity: 40.5863\n","Epoch 00029: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.6860 - perplexity: 40.5863 - val_loss: 5.2381 - val_perplexity: 193.8368\n","\n","Epoch 00030: LearningRateScheduler reducing learning rate to 0.72.\n","Epoch 30/70\n","1859/1859 [==============================] - ETA: 0s - loss: 3.6631 - perplexity: 39.6700\n","Epoch 00030: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.6631 - perplexity: 39.6700 - val_loss: 5.2472 - val_perplexity: 195.6138\n","\n","Epoch 00031: LearningRateScheduler reducing learning rate to 0.71.\n","Epoch 31/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 3.6415 - perplexity: 38.8088\n","Epoch 00031: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.6415 - perplexity: 38.8091 - val_loss: 5.2653 - val_perplexity: 199.1887\n","\n","Epoch 00032: LearningRateScheduler reducing learning rate to 0.7.\n","Epoch 32/70\n","1855/1859 [============================\u003e.] - ETA: 0s - loss: 3.6213 - perplexity: 38.0298\n","Epoch 00032: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.6216 - perplexity: 38.0426 - val_loss: 5.2822 - val_perplexity: 202.7454\n","\n","Epoch 00033: LearningRateScheduler reducing learning rate to 0.69.\n","Epoch 33/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 3.6001 - perplexity: 37.2201\n","Epoch 00033: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.6002 - perplexity: 37.2255 - val_loss: 5.2945 - val_perplexity: 205.1635\n","\n","Epoch 00034: LearningRateScheduler reducing learning rate to 0.68.\n","Epoch 34/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 3.5831 - perplexity: 36.5840\n","Epoch 00034: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.5832 - perplexity: 36.5849 - val_loss: 5.3039 - val_perplexity: 207.2070\n","\n","Epoch 00035: LearningRateScheduler reducing learning rate to 0.67.\n","Epoch 35/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 3.5633 - perplexity: 35.8749\n","Epoch 00035: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.5634 - perplexity: 35.8771 - val_loss: 5.3205 - val_perplexity: 210.7991\n","\n","Epoch 00036: LearningRateScheduler reducing learning rate to 0.66.\n","Epoch 36/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 3.5443 - perplexity: 35.1933\n","Epoch 00036: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.5444 - perplexity: 35.1957 - val_loss: 5.3305 - val_perplexity: 212.9996\n","\n","Epoch 00037: LearningRateScheduler reducing learning rate to 0.65.\n","Epoch 37/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 3.5228 - perplexity: 34.4344\n","Epoch 00037: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.5230 - perplexity: 34.4408 - val_loss: 5.3337 - val_perplexity: 213.5744\n","\n","Epoch 00038: LearningRateScheduler reducing learning rate to 0.64.\n","Epoch 38/70\n","1859/1859 [==============================] - ETA: 0s - loss: 3.5061 - perplexity: 33.8644\n","Epoch 00038: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.5061 - perplexity: 33.8644 - val_loss: 5.3418 - val_perplexity: 215.3852\n","\n","Epoch 00039: LearningRateScheduler reducing learning rate to 0.63.\n","Epoch 39/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 3.4859 - perplexity: 33.1806\n","Epoch 00039: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.4862 - perplexity: 33.1885 - val_loss: 5.3589 - val_perplexity: 219.0706\n","\n","Epoch 00040: LearningRateScheduler reducing learning rate to 0.62.\n","Epoch 40/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 3.4655 - perplexity: 32.5042\n","Epoch 00040: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.4657 - perplexity: 32.5122 - val_loss: 5.3748 - val_perplexity: 222.7724\n","\n","Epoch 00041: LearningRateScheduler reducing learning rate to 0.61.\n","Epoch 41/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 3.4459 - perplexity: 31.8669\n","Epoch 00041: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.4463 - perplexity: 31.8770 - val_loss: 5.3775 - val_perplexity: 223.2861\n","\n","Epoch 00042: LearningRateScheduler reducing learning rate to 0.6.\n","Epoch 42/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 3.4275 - perplexity: 31.2808\n","Epoch 00042: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.4276 - perplexity: 31.2821 - val_loss: 5.4028 - val_perplexity: 229.2536\n","\n","Epoch 00043: LearningRateScheduler reducing learning rate to 0.59.\n","Epoch 43/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 3.4081 - perplexity: 30.6762\n","Epoch 00043: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.4083 - perplexity: 30.6836 - val_loss: 5.4055 - val_perplexity: 229.7666\n","\n","Epoch 00044: LearningRateScheduler reducing learning rate to 0.58.\n","Epoch 44/70\n","1859/1859 [==============================] - ETA: 0s - loss: 3.3865 - perplexity: 30.0244\n","Epoch 00044: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.3865 - perplexity: 30.0244 - val_loss: 5.4220 - val_perplexity: 233.8591\n","\n","Epoch 00045: LearningRateScheduler reducing learning rate to 0.57.\n","Epoch 45/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 3.3662 - perplexity: 29.4126\n","Epoch 00045: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.3664 - perplexity: 29.4185 - val_loss: 5.4301 - val_perplexity: 235.6239\n","\n","Epoch 00046: LearningRateScheduler reducing learning rate to 0.56.\n","Epoch 46/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 3.3468 - perplexity: 28.8421\n","Epoch 00046: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.3469 - perplexity: 28.8431 - val_loss: 5.4438 - val_perplexity: 238.9301\n","\n","Epoch 00047: LearningRateScheduler reducing learning rate to 0.55.\n","Epoch 47/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 3.3244 - perplexity: 28.2010\n","Epoch 00047: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.3246 - perplexity: 28.2052 - val_loss: 5.4660 - val_perplexity: 244.4182\n","\n","Epoch 00048: LearningRateScheduler reducing learning rate to 0.54.\n","Epoch 48/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 3.3039 - perplexity: 27.6201\n","Epoch 00048: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.3042 - perplexity: 27.6272 - val_loss: 5.4773 - val_perplexity: 247.0210\n","\n","Epoch 00049: LearningRateScheduler reducing learning rate to 0.53.\n","Epoch 49/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 3.2855 - perplexity: 27.1193\n","Epoch 00049: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.2857 - perplexity: 27.1259 - val_loss: 5.4828 - val_perplexity: 248.6952\n","\n","Epoch 00050: LearningRateScheduler reducing learning rate to 0.52.\n","Epoch 50/70\n","1859/1859 [==============================] - ETA: 0s - loss: 3.2650 - perplexity: 26.5672\n","Epoch 00050: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.2650 - perplexity: 26.5672 - val_loss: 5.5036 - val_perplexity: 253.7791\n","\n","Epoch 00051: LearningRateScheduler reducing learning rate to 0.51.\n","Epoch 51/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 3.2445 - perplexity: 26.0295\n","Epoch 00051: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.2447 - perplexity: 26.0356 - val_loss: 5.5236 - val_perplexity: 259.5139\n","\n","Epoch 00052: LearningRateScheduler reducing learning rate to 0.5.\n","Epoch 52/70\n","1856/1859 [============================\u003e.] - ETA: 0s - loss: 3.2225 - perplexity: 25.4504\n","Epoch 00052: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.2227 - perplexity: 25.4559 - val_loss: 5.5333 - val_perplexity: 262.0270\n","\n","Epoch 00053: LearningRateScheduler reducing learning rate to 0.49.\n","Epoch 53/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 3.2017 - perplexity: 24.9244\n","Epoch 00053: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.2019 - perplexity: 24.9279 - val_loss: 5.5430 - val_perplexity: 264.1765\n","\n","Epoch 00054: LearningRateScheduler reducing learning rate to 0.48.\n","Epoch 54/70\n","1859/1859 [==============================] - ETA: 0s - loss: 3.1841 - perplexity: 24.4877\n","Epoch 00054: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.1841 - perplexity: 24.4877 - val_loss: 5.5602 - val_perplexity: 269.2825\n","\n","Epoch 00055: LearningRateScheduler reducing learning rate to 0.47.\n","Epoch 55/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 3.1602 - perplexity: 23.9055\n","Epoch 00055: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.1602 - perplexity: 23.9055 - val_loss: 5.5737 - val_perplexity: 272.5118\n","\n","Epoch 00056: LearningRateScheduler reducing learning rate to 0.46.\n","Epoch 56/70\n","1859/1859 [==============================] - ETA: 0s - loss: 3.1342 - perplexity: 23.2869\n","Epoch 00056: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.1342 - perplexity: 23.2869 - val_loss: 5.5827 - val_perplexity: 275.3590\n","\n","Epoch 00057: LearningRateScheduler reducing learning rate to 0.45.\n","Epoch 57/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 3.1125 - perplexity: 22.7868\n","Epoch 00057: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.1127 - perplexity: 22.7903 - val_loss: 5.6051 - val_perplexity: 281.9591\n","\n","Epoch 00058: LearningRateScheduler reducing learning rate to 0.44.\n","Epoch 58/70\n","1859/1859 [==============================] - ETA: 0s - loss: 3.0905 - perplexity: 22.2858\n","Epoch 00058: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.0905 - perplexity: 22.2858 - val_loss: 5.6142 - val_perplexity: 284.6101\n","\n","Epoch 00059: LearningRateScheduler reducing learning rate to 0.43.\n","Epoch 59/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 3.0666 - perplexity: 21.7608\n","Epoch 00059: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.0667 - perplexity: 21.7614 - val_loss: 5.6410 - val_perplexity: 292.4486\n","\n","Epoch 00060: LearningRateScheduler reducing learning rate to 0.42.\n","Epoch 60/70\n","1859/1859 [==============================] - ETA: 0s - loss: 3.0434 - perplexity: 21.2600\n","Epoch 00060: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 3.0434 - perplexity: 21.2600 - val_loss: 5.6541 - val_perplexity: 296.2208\n","\n","Epoch 00061: LearningRateScheduler reducing learning rate to 0.41.\n","Epoch 61/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 3.0184 - perplexity: 20.7321\n","Epoch 00061: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 24s 13ms/step - loss: 3.0185 - perplexity: 20.7346 - val_loss: 5.6625 - val_perplexity: 298.7459\n","\n","Epoch 00062: LearningRateScheduler reducing learning rate to 0.4.\n","Epoch 62/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 2.9933 - perplexity: 20.2122\n","Epoch 00062: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 2.9933 - perplexity: 20.2126 - val_loss: 5.6874 - val_perplexity: 307.0420\n","\n","Epoch 00063: LearningRateScheduler reducing learning rate to 0.39.\n","Epoch 63/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 2.9676 - perplexity: 19.6989\n","Epoch 00063: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 2.9677 - perplexity: 19.7009 - val_loss: 5.7045 - val_perplexity: 312.4807\n","\n","Epoch 00064: LearningRateScheduler reducing learning rate to 0.38.\n","Epoch 64/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 2.9433 - perplexity: 19.2217\n","Epoch 00064: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 2.9433 - perplexity: 19.2222 - val_loss: 5.7243 - val_perplexity: 318.4995\n","\n","Epoch 00065: LearningRateScheduler reducing learning rate to 0.37.\n","Epoch 65/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 2.9146 - perplexity: 18.6777\n","Epoch 00065: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 2.9147 - perplexity: 18.6786 - val_loss: 5.7440 - val_perplexity: 324.8350\n","\n","Epoch 00066: LearningRateScheduler reducing learning rate to 0.36.\n","Epoch 66/70\n","1859/1859 [==============================] - ETA: 0s - loss: 2.8888 - perplexity: 18.1987\n","Epoch 00066: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 2.8888 - perplexity: 18.1987 - val_loss: 5.7621 - val_perplexity: 330.5020\n","\n","Epoch 00067: LearningRateScheduler reducing learning rate to 0.35.\n","Epoch 67/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 2.8623 - perplexity: 17.7216\n","Epoch 00067: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 2.8623 - perplexity: 17.7228 - val_loss: 5.7827 - val_perplexity: 337.7800\n","\n","Epoch 00068: LearningRateScheduler reducing learning rate to 0.34.\n","Epoch 68/70\n","1858/1859 [============================\u003e.] - ETA: 0s - loss: 2.8381 - perplexity: 17.2925\n","Epoch 00068: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 2.8381 - perplexity: 17.2929 - val_loss: 5.7970 - val_perplexity: 342.5734\n","\n","Epoch 00069: LearningRateScheduler reducing learning rate to 0.33.\n","Epoch 69/70\n","1857/1859 [============================\u003e.] - ETA: 0s - loss: 2.8121 - perplexity: 16.8456\n","Epoch 00069: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 2.8121 - perplexity: 16.8468 - val_loss: 5.8209 - val_perplexity: 351.3444\n","\n","Epoch 00070: LearningRateScheduler reducing learning rate to 0.32.\n","Epoch 70/70\n","1859/1859 [==============================] - ETA: 0s - loss: 2.7839 - perplexity: 16.3788\n","Epoch 00070: val_loss did not improve from 4.99382\n","1859/1859 [==============================] - 25s 13ms/step - loss: 2.7839 - perplexity: 16.3788 - val_loss: 5.8386 - val_perplexity: 357.4742\n"]}],"source":["history = {}\r\n","for model_def in model_defs:\r\n","  model = create_compile(model_def)\r\n","  model.summary()\r\n","  fit_hist = model.fit(x=dataset_train,\r\n","                       validation_data=dataset_valid,\r\n","                       batch_size=batch_size,\r\n","                       epochs=epochs,\r\n","                       callbacks=get_callbacks(model.name))\r\n","  history[model_def['name']] = fit_hist.history\r\n","  model.save(model.name + '.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XY1gFczYKKqM"},"outputs":[],"source":["with open('trainHistoryLSTM', 'wb') as file:\r\n","        pickle.dump(history, file)"]},{"cell_type":"markdown","metadata":{"id":"jy_MyJASxzpZ"},"source":["## Re-Load Models with best weights and evaluate:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQR74bG_xy47"},"outputs":[],"source":["results = {}\r\n","for model_def in model_defs:\r\n","  model = create_compile(model_def)\r\n","  model.load_weights(model.name + '.weights.hdf5')\r\n","\r\n","  cur_res = {}\r\n","  [cur_res['train_loss'], cur_res['train_perp']] = model.evaluate(dataset_train)\r\n","  [cur_res['valid_loss'], cur_res['valid_perp']] = model.evaluate(dataset_valid)\r\n","  [cur_res['test_loss'], cur_res['test_perp']] = model.evaluate(dataset_test)\r\n","  results[model.name] = cur_res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttsO6FQK9cHc"},"outputs":[],"source":["%tensorboard --logdir tensorboard_logs/"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["2ybPw3GS_3mq","-HDaq6iAG8nv","WZBSBIKs0TY4"],"name":"Train_LSTM.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}